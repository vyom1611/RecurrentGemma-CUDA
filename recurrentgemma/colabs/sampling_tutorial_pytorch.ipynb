{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dHtP_crNpBn"
      },
      "source": [
        "Copyright 2024 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPxT0O0sNpBn"
      },
      "source": [
        "# Getting Started with Recurrent Gemma Sampling: A Step-by-Step Guide\n",
        "\n",
        "You will find in this colab a detailed tutorial explaining how to load a Recurrent Gemma checkpoint and sample from it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bCVMxMVNpBn"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3yc0pyXNpBn"
      },
      "outputs": [],
      "source": [
        "! pip install git+https://github.com/google-deepmind/recurrentgemma.git#egg=recurrentgemma[torch]\n",
        "! pip install --user kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8Gz-SmzNpBn"
      },
      "source": [
        "## Downloading the checkpoint\n",
        "\n",
        "\"To use Gemma's checkpoints, you'll need a Kaggle account and API key. Here's how to get them:\n",
        "\n",
        "1. Visit https://www.kaggle.com/ and create an account.\n",
        "2. Go to your account settings, then the 'API' section.\n",
        "3. Click 'Create new token' to download your key.\n",
        "4. You can either login using the UI interface or by setting your Kaggle username and key via the Colab secrets.\n",
        "\n",
        "Then run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7TzksBbNpBn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import kagglehub\n",
        "\n",
        "try:\n",
        "  os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n",
        "  os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
        "except userdata.SecretNotFoundError:\n",
        "  kagglehub.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-3QmVOqNpBn"
      },
      "source": [
        "If everything went well, you should see:\n",
        "```\n",
        "Kaggle credentials set.\n",
        "Kaggle credentials successfully validated.\n",
        "```\n",
        "\n",
        "Now select and download the checkpoint you want to try. Note only the '2b-it' and '9b-it' checkpoint has been tuned for chat and question answering. The '2b' and '9b' checkpoints have only been trained for next token prediction so will not perform as well in a \"chat\" or \"QA\" setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF4W_TTINpBn"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import pathlib\n",
        "import torch\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "from recurrentgemma import torch as recurrentgemma\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB-4v2sQNpBn"
      },
      "outputs": [],
      "source": [
        "VARIANT = '2b-it' # @param ['2b', '2b-it', '9b', '9b-it'] {type:\"string\"}\n",
        "weights_dir = kagglehub.model_download(f'google/recurrentgemma/pyTorch/{VARIANT}')\n",
        "\n",
        "weights_dir = pathlib.Path(weights_dir)\n",
        "ckpt_path = weights_dir / f'{VARIANT}.pt'\n",
        "vocab_path = weights_dir / 'tokenizer.model'\n",
        "preset = recurrentgemma.Preset.RECURRENT_GEMMA_2B_V1 if '2b' in VARIANT else recurrentgemma.Preset.RECURRENT_GEMMA_9B_V1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ftQWMeRNpBn"
      },
      "source": [
        "## Start Generating with Your Model\n",
        "\n",
        "Load and prepare your LLM's checkpoint for use with Flax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_2NisxHNpBn"
      },
      "outputs": [],
      "source": [
        "# Load parameters\n",
        "params = torch.load(str(ckpt_path))\n",
        "params = {k : v.to(device=device) for k, v in params.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mel6IkR-NpBn"
      },
      "source": [
        "Use the `griffin_lib.GriffinConfig.from_torch_params` function to automatically load the correct configuration from a checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBAcfETaNpBn"
      },
      "outputs": [],
      "source": [
        "model_config = recurrentgemma.GriffinConfig.from_torch_params(\n",
        "    params,\n",
        "    preset=preset,\n",
        ")\n",
        "model = recurrentgemma.Griffin(model_config, device=device, dtype=torch.bfloat16)\n",
        "model.load_state_dict(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZdN8X5HNpBn"
      },
      "source": [
        "Load your tokenizer, which we'll construct using the [SentencePiece](https://github.com/google/sentencepiece) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R25QSS_xNpBn"
      },
      "outputs": [],
      "source": [
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.Load(str(vocab_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExH48s5FNpBn"
      },
      "source": [
        "Finally, build a sampler on top of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ps-2B-SNpBn"
      },
      "outputs": [],
      "source": [
        "sampler = recurrentgemma.Sampler(model=model, vocab=vocab, is_it_model=\"it\" in VARIANT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGc4B3xYNpBn"
      },
      "source": [
        "You're ready to start sampling ! This sampler uses just-in-time compilation, so changing the input shape triggers recompilation, which can slow things down. For the fastest and most efficient results, keep your batch size consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq6yK9yTNpBn"
      },
      "outputs": [],
      "source": [
        "input_batch = [\n",
        "  \"What are the planets of the solar system?\",\n",
        "]\n",
        "\n",
        "# 300 generation steps\n",
        "out_data = sampler(input_strings=input_batch, total_generation_steps=300)\n",
        "\n",
        "for input_string, out_string in zip(input_batch, out_data.text):\n",
        "  print(f\"Prompt:\\n{input_string}\\nOutput:\\n{out_string}\")\n",
        "  print(10*'#')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i15odZWiNpBn"
      },
      "source": [
        "You should get a description of the solar system.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {},
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
